{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 3: Model Operationalization & Deployment\n",
    "\n",
    "In this script, a model is saved as a .model file along with the relevant scheme for deployment. The functions are first tested locally before operationalizing the model using Azure Machine Learning Model Management environment for use in production in realtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Setup the pyspark environment\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Setting seed for reproducability\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "%matplotlib inline\n",
    "\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load feature data set\n",
    "\n",
    "We have previously created the labeled feature data set in the `Code\\1_data_ingestion_and_preparation.ipynb` Jupyter notebook. Since the Azure Blob storage account name and account key are not passed between notebooks, you'll need your credentials here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Unable to infer schema for Parquet. It must be specified manually.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.parquet.\n: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:182)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:182)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:181)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:559)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5c98b84be0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0maz_blob_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_blob_to_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTAINER_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFE_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m#test_df.limit(5).toPandas().head(5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Unable to infer schema for Parquet. It must be specified manually.;'"
     ]
    }
   ],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"<your blob storage account key>\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the Data Ingestion and Preparation notebook is stored in the sensordata ingestion container.\n",
    "CONTAINER_NAME = 'sensordataingestiontest'\n",
    "FE_DIRECTORY = 'PM_test_files.parquet'\n",
    "#FE_DIRECTORY = 'sensordataingestiontest_files.parquet'\n",
    "\n",
    "MODEL_CONTAINER = 'modeldeploy'\n",
    "\n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(CONTAINER_NAME, \n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# create a local path where to store the results later.\n",
    "if not os.path.exists(FE_DIRECTORY):\n",
    "    os.makedirs(FE_DIRECTORY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if CONTAINER_NAME in blob.name:\n",
    "        local_file = os.path.join(FE_DIRECTORY, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "test_df = spark.read.parquet(FE_DIRECTORY)\n",
    "#test_df.limit(5).toPandas().head(5)\n",
    "\n",
    "test_df = test_df.toPandas()\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define init and run functions\n",
    "Start by defining the init() and run() functions as shown in the cell below. Then write them to the score.py file. This file will load the model, perform the prediction, and return the result.\n",
    "\n",
    "The init() function initializes your web service, loading in any data or models that you need to score your inputs. In the example below, we load in the trained model. This command is run when the Docker container containing your service initializes.\n",
    "The run() function defines what is executed on a scoring call. In our simple example, we simply load in the input as a data frame, and run our pipeline on the input, and return the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    global pipeline\n",
    "    \n",
    "    pipeline = PipelineModel.load(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]    \n",
    "    # pick the feature columns \n",
    "    sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "        \n",
    "    # generator for the sequences\n",
    "    seq_gen = (list(gen_sequence(test_df[test_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in test_df['id'].unique())\n",
    "\n",
    "    # generate sequences and convert to numpy array\n",
    "    seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "    seq_array.shape\n",
    "\n",
    "def run(input_df):\n",
    "    import json\n",
    "    response = ''\n",
    "    try:\n",
    "        score = pipeline.transform(seq_array)\n",
    "        predictions = score.collect()   \n",
    "\n",
    "   \n",
    "        # Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\", str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "        # Return results\n",
    "        print(json.dumps(response))\n",
    "        return json.dumps(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create schema and schema file\n",
    "Create a schema for the input to the web service and generate the schema file. This will be used to create a Swagger file for your web service which can be used to discover its input and sample data when calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b89d6271fa5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define the input data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_df\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSampleDefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPANDAS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgenerate_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'service_schema.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"input_df\": SampleDefinition(DataTypes.PANDAS, test_df)}\n",
    "generate_schema(run_func=run, inputs=inputs, filepath='service_schema.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'score' from 'C:\\\\Users\\\\lazzeri\\\\AppData\\\\Local\\\\Temp\\\\azureml_runs\\\\lstmpm_v2_1510243980846\\\\score.py'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "import score\n",
    "imp.reload(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"        id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\\\\\\\\\\n0        1      1  0.459770  0.166667       0.0  0.0  0.183735  0.406802   \\\\n1        1      2  0.609195  0.250000       0.0  0.0  0.283133  0.453019   \\\\n2        1      3  0.252874  0.750000       0.0  0.0  0.343373  0.369523   \\\\n3        1      4  0.540230  0.500000       0.0  0.0  0.343373  0.256159   \\\\n4        1      5  0.390805  0.333333       0.0  0.0  0.349398  0.257467   \\\\n5        1      6  0.252874  0.416667       0.0  0.0  0.268072  0.292784   \\\\n6        1      7  0.557471  0.583333       0.0  0.0  0.382530  0.463920   \\\\n7        1      8  0.304598  0.750000       0.0  0.0  0.406627  0.259865   \\\\n8        1      9  0.545977  0.583333       0.0  0.0  0.274096  0.434707   \\\\n9        1     10  0.310345  0.583333       0.0  0.0  0.150602  0.440375   \\\\n10       1     11  0.603448  0.250000       0.0  0.0  0.322289  0.233486   \\\\n11       1     12  0.591954  0.666667       0.0  0.0  0.256024  0.269675   \\\\n12       1     13  0.390805  0.833333       0.0  0.0  0.560241  0.243078   \\\\n13       1     14  0.551724  0.500000       0.0  0.0  0.343373  0.477654   \\\\n14       1     15  0.396552  0.250000       0.0  0.0  0.367470  0.278613   \\\\n15       1     16  0.534483  0.916667       0.0  0.0  0.277108  0.369305   \\\\n16       1     17  0.511494  0.666667       0.0  0.0  0.412651  0.303466   \\\\n17       1     18  0.321839  0.416667       0.0  0.0  0.424699  0.436015   \\\\n18       1     19  0.683908  0.250000       0.0  0.0  0.174699  0.360148   \\\\n19       1     20  0.287356  0.583333       0.0  0.0  0.551205  0.219533   \\\\n20       1     21  0.431034  0.583333       0.0  0.0  0.349398  0.327665   \\\\n21       1     22  0.511494  0.500000       0.0  0.0  0.469880  0.477218   \\\\n22       1     23  0.695402  0.250000       0.0  0.0  0.280120  0.373883   \\\\n23       1     24  0.442529  0.750000       0.0  0.0  0.352410  0.431437   \\\\n24       1     25  0.632184  0.166667       0.0  0.0  0.469880  0.502725   \\\\n25       1     26  0.500000  0.666667       0.0  0.0  0.286145  0.393285   \\\\n26       1     27  0.431034  0.166667       0.0  0.0  0.370482  0.423588   \\\\n27       1     28  0.362069  0.916667       0.0  0.0  0.343373  0.257249   \\\\n28       1     29  0.568966  0.416667       0.0  0.0  0.210843  0.300632   \\\\n29       1     30  0.373563  0.500000       0.0  0.0  0.298193  0.490081   \\\\n...    ...    ...       ...       ...       ...  ...       ...       ...   \\\\n20601  100    171  0.471264  0.166667       0.0  0.0  0.554217  0.490953   \\\\n20602  100    172  0.287356  0.583333       0.0  0.0  0.530120  0.682581   \\\\n20603  100    173  0.534483  0.833333       0.0  0.0  0.659639  0.533900   \\\\n20604  100    174  0.563218  0.666667       0.0  0.0  0.512048  0.680183   \\\\n20605  100    175  0.425287  0.083333       0.0  0.0  0.493976  0.675605   \\\\n20606  100    176  0.402299  0.250000       0.0  0.0  0.638554  0.662743   \\\\n20607  100    177  0.436782  0.083333       0.0  0.0  0.641566  0.483540   \\\\n20608  100    178  0.528736  0.250000       0.0  0.0  0.533133  0.517986   \\\\n20609  100    179  0.614943  0.833333       0.0  0.0  0.605422  0.617397   \\\\n20610  100    180  0.442529  0.583333       0.0  0.0  0.731928  0.543710   \\\\n20611  100    181  0.637931  0.083333       0.0  0.0  0.614458  0.584042   \\\\n20612  100    182  0.540230  0.416667       0.0  0.0  0.695783  0.725311   \\\\n20613  100    183  0.436782  0.333333       0.0  0.0  0.641566  0.513625   \\\\n20614  100    184  0.655172  0.166667       0.0  0.0  0.512048  0.606933   \\\\n20615  100    185  0.419540  0.833333       0.0  0.0  0.825301  0.649008   \\\\n20616  100    186  0.649425  0.833333       0.0  0.0  0.722892  0.490735   \\\\n20617  100    187  0.586207  0.666667       0.0  0.0  0.728916  0.565075   \\\\n20618  100    188  0.454023  0.333333       0.0  0.0  0.596386  0.582734   \\\\n20619  100    189  0.586207  0.583333       0.0  0.0  0.746988  0.628079   \\\\n20620  100    190  0.494253  0.666667       0.0  0.0  0.575301  0.510355   \\\\n20621  100    191  0.471264  0.500000       0.0  0.0  0.746988  0.868324   \\\\n20622  100    192  0.448276  0.583333       0.0  0.0  0.698795  0.658164   \\\\n20623  100    193  0.494253  0.666667       0.0  0.0  0.566265  0.627207   \\\\n20624  100    194  0.436782  0.750000       0.0  0.0  0.756024  0.572269   \\\\n20625  100    195  0.488506  0.416667       0.0  0.0  0.662651  0.632221   \\\\n20626  100    196  0.477011  0.250000       0.0  0.0  0.686747  0.587312   \\\\n20627  100    197  0.408046  0.083333       0.0  0.0  0.701807  0.729453   \\\\n20628  100    198  0.522989  0.500000       0.0  0.0  0.665663  0.684979   \\\\n20629  100    199  0.436782  0.750000       0.0  0.0  0.608434  0.746021   \\\\n20630  100    200  0.316092  0.083333       0.0  0.0  0.795181  0.639634   \\\\n\\\\n             s4   s5     ...      s16       s17  s18  s19       s20       s21  \\\\\\\\\\\\n0      0.309757  0.0     ...      0.0  0.333333  0.0  0.0  0.713178  0.724662   \\\\n1      0.352633  0.0     ...      0.0  0.333333  0.0  0.0  0.666667  0.731014   \\\\n2      0.370527  0.0     ...      0.0  0.166667  0.0  0.0  0.627907  0.621375   \\\\n3      0.331195  0.0     ...      0.0  0.333333  0.0  0.0  0.573643  0.662386   \\\\n4      0.404625  0.0     ...      0.0  0.416667  0.0  0.0  0.589147  0.704502   \\\\n5      0.272113  0.0     ...      0.0  0.250000  0.0  0.0  0.651163  0.652720   \\\\n6      0.261985  0.0     ...      0.0  0.333333  0.0  0.0  0.744186  0.667219   \\\\n7      0.316003  0.0     ...      0.0  0.250000  0.0  0.0  0.643411  0.574979   \\\\n8      0.211850  0.0     ...      0.0  0.333333  0.0  0.0  0.705426  0.707539   \\\\n9      0.307394  0.0     ...      0.0  0.416667  0.0  0.0  0.627907  0.794256   \\\\n10     0.310432  0.0     ...      0.0  0.333333  0.0  0.0  0.620155  0.807097   \\\\n11     0.302161  0.0     ...      0.0  0.250000  0.0  0.0  0.713178  0.651477   \\\\n12     0.313639  0.0     ...      0.0  0.416667  0.0  0.0  0.612403  0.526788   \\\\n13     0.285449  0.0     ...      0.0  0.416667  0.0  0.0  0.806202  0.674399   \\\\n14     0.335584  0.0     ...      0.0  0.250000  0.0  0.0  0.658915  0.629384   \\\\n15     0.375591  0.0     ...      0.0  0.333333  0.0  0.0  0.643411  0.774372   \\\\n16     0.298785  0.0     ...      0.0  0.333333  0.0  0.0  0.519380  0.604391   \\\\n17     0.234132  0.0     ...      0.0  0.333333  0.0  0.0  0.581395  0.696631   \\\\n18     0.305537  0.0     ...      0.0  0.250000  0.0  0.0  0.511628  0.624413   \\\\n19     0.387914  0.0     ...      0.0  0.333333  0.0  0.0  0.689922  0.728804   \\\\n20     0.268062  0.0     ...      0.0  0.333333  0.0  0.0  0.736434  0.574289   \\\\n21     0.309251  0.0     ...      0.0  0.333333  0.0  0.0  0.604651  0.669705   \\\\n22     0.211006  0.0     ...      0.0  0.333333  0.0  0.0  0.620155  0.776029   \\\\n23     0.279541  0.0     ...      0.0  0.333333  0.0  0.0  0.666667  0.656448   \\\\n24     0.289332  0.0     ...      0.0  0.416667  0.0  0.0  0.627907  0.738194   \\\\n25     0.233288  0.0     ...      0.0  0.500000  0.0  0.0  0.558140  0.719000   \\\\n26     0.330689  0.0     ...      0.0  0.416667  0.0  0.0  0.658915  0.763601   \\\\n27     0.284943  0.0     ...      0.0  0.166667  0.0  0.0  0.674419  0.538387   \\\\n28     0.316340  0.0     ...      0.0  0.416667  0.0  0.0  0.612403  0.642778   \\\\n29     0.233457  0.0     ...      0.0  0.166667  0.0  0.0  0.705426  0.713615   \\\\n...         ...  ...     ...      ...       ...  ...  ...       ...       ...   \\\\n20601  0.645341  0.0     ...      0.0  0.666667  0.0  0.0  0.519380  0.355012   \\\\n20602  0.720459  0.0     ...      0.0  0.500000  0.0  0.0  0.271318  0.411489   \\\\n20603  0.614112  0.0     ...      0.0  0.416667  0.0  0.0  0.217054  0.393676   \\\\n20604  0.730419  0.0     ...      0.0  0.583333  0.0  0.0  0.511628  0.392433   \\\\n20605  0.573768  0.0     ...      0.0  0.500000  0.0  0.0  0.310078  0.244546   \\\\n20606  0.660871  0.0     ...      0.0  0.666667  0.0  0.0  0.248062  0.286937   \\\\n20607  0.618839  0.0     ...      0.0  0.583333  0.0  0.0  0.356589  0.194698   \\\\n20608  0.682647  0.0     ...      0.0  0.583333  0.0  0.0  0.317829  0.457056   \\\\n20609  0.703747  0.0     ...      0.0  0.666667  0.0  0.0  0.372093  0.378763   \\\\n20610  0.577313  0.0     ...      0.0  0.583333  0.0  0.0  0.310078  0.469898   \\\\n20611  0.546590  0.0     ...      0.0  0.666667  0.0  0.0  0.294574  0.391052   \\\\n20612  0.598920  0.0     ...      0.0  0.666667  0.0  0.0  0.209302  0.324082   \\\\n20613  0.759959  0.0     ...      0.0  0.583333  0.0  0.0  0.271318  0.243993   \\\\n20614  0.652262  0.0     ...      0.0  0.666667  0.0  0.0  0.232558  0.315797   \\\\n20615  0.642978  0.0     ...      0.0  0.583333  0.0  0.0  0.356589  0.439796   \\\\n20616  0.727043  0.0     ...      0.0  0.500000  0.0  0.0  0.286822  0.308064   \\\\n20617  0.662390  0.0     ...      0.0  0.666667  0.0  0.0  0.410853  0.464789   \\\\n20618  0.748143  0.0     ...      0.0  0.583333  0.0  0.0  0.170543  0.222314   \\\\n20619  0.690412  0.0     ...      0.0  0.750000  0.0  0.0  0.395349  0.227700   \\\\n20620  0.739196  0.0     ...      0.0  0.583333  0.0  0.0  0.217054  0.229357   \\\\n20621  0.758609  0.0     ...      0.0  0.833333  0.0  0.0  0.193798  0.314278   \\\\n20622  0.628460  0.0     ...      0.0  0.750000  0.0  0.0  0.325581  0.252416   \\\\n20623  0.787981  0.0     ...      0.0  0.750000  0.0  0.0  0.255814  0.177851   \\\\n20624  0.762323  0.0     ...      0.0  0.500000  0.0  0.0  0.186047  0.328915   \\\\n20625  0.838116  0.0     ...      0.0  0.500000  0.0  0.0  0.000000  0.411627   \\\\n20626  0.782917  0.0     ...      0.0  0.750000  0.0  0.0  0.271318  0.109500   \\\\n20627  0.866475  0.0     ...      0.0  0.583333  0.0  0.0  0.124031  0.366197   \\\\n20628  0.775321  0.0     ...      0.0  0.833333  0.0  0.0  0.232558  0.053991   \\\\n20629  0.747468  0.0     ...      0.0  0.583333  0.0  0.0  0.116279  0.234466   \\\\n20630  0.842167  0.0     ...      0.0  0.666667  0.0  0.0  0.178295  0.218172   \\\\n\\\\n       RUL  label1  label2  cycle_norm  \\\\n0      191       0       0    0.000000  \\\\n1      190       0       0    0.002770  \\\\n2      189       0       0    0.005540  \\\\n3      188       0       0    0.008310  \\\\n4      187       0       0    0.011080  \\\\n5      186       0       0    0.013850  \\\\n6      185       0       0    0.016620  \\\\n7      184       0       0    0.019391  \\\\n8      183       0       0    0.022161  \\\\n9      182       0       0    0.024931  \\\\n10     181       0       0    0.027701  \\\\n11     180       0       0    0.030471  \\\\n12     179       0       0    0.033241  \\\\n13     178       0       0    0.036011  \\\\n14     177       0       0    0.038781  \\\\n15     176       0       0    0.041551  \\\\n16     175       0       0    0.044321  \\\\n17     174       0       0    0.047091  \\\\n18     173       0       0    0.049861  \\\\n19     172       0       0    0.052632  \\\\n20     171       0       0    0.055402  \\\\n21     170       0       0    0.058172  \\\\n22     169       0       0    0.060942  \\\\n23     168       0       0    0.063712  \\\\n24     167       0       0    0.066482  \\\\n25     166       0       0    0.069252  \\\\n26     165       0       0    0.072022  \\\\n27     164       0       0    0.074792  \\\\n28     163       0       0    0.077562  \\\\n29     162       0       0    0.080332  \\\\n...    ...     ...     ...         ...  \\\\n20601   29       1       1    0.470914  \\\\n20602   28       1       1    0.473684  \\\\n20603   27       1       1    0.476454  \\\\n20604   26       1       1    0.479224  \\\\n20605   25       1       1    0.481994  \\\\n20606   24       1       1    0.484765  \\\\n20607   23       1       1    0.487535  \\\\n20608   22       1       1    0.490305  \\\\n20609   21       1       1    0.493075  \\\\n20610   20       1       1    0.495845  \\\\n20611   19       1       1    0.498615  \\\\n20612   18       1       1    0.501385  \\\\n20613   17       1       1    0.504155  \\\\n20614   16       1       1    0.506925  \\\\n20615   15       1       2    0.509695  \\\\n20616   14       1       2    0.512465  \\\\n20617   13       1       2    0.515235  \\\\n20618   12       1       2    0.518006  \\\\n20619   11       1       2    0.520776  \\\\n20620   10       1       2    0.523546  \\\\n20621    9       1       2    0.526316  \\\\n20622    8       1       2    0.529086  \\\\n20623    7       1       2    0.531856  \\\\n20624    6       1       2    0.534626  \\\\n20625    5       1       2    0.537396  \\\\n20626    4       1       2    0.540166  \\\\n20627    3       1       2    0.542936  \\\\n20628    2       1       2    0.545706  \\\\n20629    1       1       2    0.548476  \\\\n20630    0       1       2    0.551247  \\\\n\\\\n[20631 rows x 30 columns] 42\"'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.init()\n",
    "score.run(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model assets\n",
    "Next we persist the assets we have created to disk for use in operationalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the schema file for deployment\n",
    "out = json.dumps(json_schema)\n",
    "with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'service_schema.json', 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use %%writefile meta command to save the init() and run() functions to the save the pdmscore.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile {os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']}/pdmscore.py\n",
    "\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "# for creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    # read in the model file\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load('pdmrfull.model')\n",
    "    \n",
    "def run(input_df):\n",
    "    response = ''\n",
    "    try:\n",
    "        score = pipeline.transform(seq_array)\n",
    "        predictions = score.collect() \n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are stored in the ['AZUREML_NATIVE_SHARE_DIRECTORY'] location on the kernel host machine with the model stored in the `Code\\3_model_building_and_evaluation.ipynb` notebook. In order to share these assets and operationalize the model, we create a new blob container and store a compressed file containing those assets for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compress the operationalization assets for easy blob storage transfer\n",
    "MODEL_O16N = shutil.make_archive('o16n', 'zip', os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'])\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(MODEL_CONTAINER, \n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# Transfer the compressed operationalization assets into the blob container.\n",
    "az_blob_service.create_blob_from_path(MODEL_CONTAINER, \"o16n.zip\", str(MODEL_O16N) ) \n",
    "\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run All\" cells\n",
    "toc = time.time()\n",
    "print(\"Full run took %.2f minutes\" % ((toc - tic)/60))\n",
    "\n",
    "logger.log(\"Operationalization Run time\", ((toc - tic)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Once the assets are stored, we can download them into a local compute context for operationalization on an Azure web service.\n",
    "\n",
    "We demonstrate how to setup this web service this through a CLI window opened in the AML Workbench application. \n",
    "\n",
    "## Download the model\n",
    "\n",
    "To download the model we've saved, follow these instructions on a local computer.\n",
    "\n",
    "- Open the [Azure Portal](http://portal.azure.com)\n",
    "- In the left hand pane, click on __All resources__\n",
    "- Search for the storage account using the name you provided earlier in this notebook. \n",
    "- Choose the storage account from search result list, this will open the storage account panel.\n",
    "- On the storage account panel, choose __Blobs__\n",
    "- On the Blobs panel choose the container __modeldeploy__\n",
    "- Select the file o16n.zip and on the properties pane for that blob choose download.\n",
    "\n",
    "Once downloaded, unzip the file into the directory of your choosing. The zip file contains three deployment assets:\n",
    "\n",
    "- the `lstmscore.py` file\n",
    "- a `lstm.model` directory\n",
    "- the `service_schema.json` file\n",
    "\n",
    "\n",
    "\n",
    "## Create a model management endpoint \n",
    "\n",
    "Create a modelmanagement under your account. We will call this `lstmmodelmanagement`. The remaining defaults are acceptable.\n",
    "\n",
    "`az ml account modelmanagement create --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "\n",
    "## Check environment settings\n",
    "\n",
    "Show what environment is currently active:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "If nothing is set, we setup the environment with the existing model management context first: \n",
    "\n",
    "` az ml env setup --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "then set the current environment:\n",
    "\n",
    "`az ml env set --resource-group <RESOURCE_GROUP> --cluster-name pdmmodelmanagement`\n",
    "\n",
    "Check that the environment is now set:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "\n",
    "## Deploy your web service \n",
    "\n",
    "Once the environment is setup, we'll deploy the web service from the CLI.\n",
    "\n",
    "These commands assume the current directory contains the webservice assets we created in throughout the notebooks in this scenario (`lstmscore.py`, `service_schema.json` and `lstm.model`). If your kernel has run locally, the assets will be in the `os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']`. \n",
    "\n",
    "On windows this points to:\n",
    "\n",
    "```\n",
    "cd C:\\Users\\<username>\\.azureml\\share\\<team account>\\<Project Name>\n",
    "```\n",
    "\n",
    "on linux variants this points to:\n",
    "\n",
    "```\n",
    "cd ~\\.azureml\\share\\<team account>\\<Project Name>\n",
    "```\n",
    "\n",
    "\n",
    "The command to create a web service (`<SERVICE_ID>`) with these operationalization assets in the current directory is:\n",
    "\n",
    "```\n",
    "az ml service create realtime -f <filename> -r <TARGET_RUNTIME> -m <MODEL_FILE> -s <SCHEMA_FILE> -n <SERVICE_ID> --cpu 0.1\n",
    "```\n",
    "\n",
    "The default cluster has only 2 nodes with 2 cores each. Some cores are taken for system components. AMLWorkbench asks for 1 core per service. To deploy multiple services into this cluster, we specify the cpu requirement in the service create command as (--cpu 0.1) to request 10% of a core. \n",
    "\n",
    "For this example, we will call our webservice `amlworkbenchpdmwebservice`. This `SERVICE_ID` must be all lowercase, with no spaces:\n",
    "\n",
    "```\n",
    "az ml service create realtime -f lstmscore.py -r spark-py -m lstm.model -s service_schema.json --cpu 0.1 -n amlworkbenchpdmwebservice\n",
    "```\n",
    "\n",
    "This command will take some time to execute. \n",
    "\n",
    "Once complete, the command returns sample usage commands to test the service for both PowerShell and the cmd prompt. We can execute these commands from the command line as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DeepLearningforPM DeepLearningforPM",
   "language": "python",
   "name": "deeplearningforpm_deeplearningforpm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
