{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predictive Maintenance with LSTM Deep Learning Models\n",
    "\n",
    "This notebook is based on the [example](https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb) created by Fidan and Francesca. It uses [keras](https://keras.io/) deep learning library with Microsoft Cognitive Toolkit [CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras) or TensorFlow as backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components\n",
    "import os\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"cntk\"\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting seed for reproducability\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from math import ceil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "SAMPLE_MODULE_TYPE = 1 \n",
    "FORESIGHT_LEN_HOUR = 8\n",
    "SEQUENCE_LEN_LSTM = 48 \n",
    "PROBABILITY_TH = 0.1\n",
    "SEQUENCE_COLS_LSTM = ['FF DRS', 'month_of_year', 'week_of_month', 'day_of_week']\n",
    "PREDICT_HAPPEN_DT = \"2017-05-28 12:00:00\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "In this part, we prepare data for training LSTM predictive maintenance model. First, we generate new labels indicating whether an alarm will happen in the next few hours. Then, we split the data time column 'rests_hourly' into multiple columns. After this, we partition the data into training and testing sets. We also generate an 'id' column to uniquely identify each device. This column and the 'rests_hourly' column have similar meaning to the 'id' column and 'cycle' column in the referenced example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/azureml-run\n",
      "sensor_data_modtype52_count1.csv\n",
      "sensor_data_modtype2_count5.csv\n",
      "sensor_data_modtype71_count7.csv\n",
      "sensor_data_modtype37_count2.csv\n",
      "sensor_data_modtype31_count26.csv\n",
      "sensor_data_modtype22_count4.csv\n",
      "sensor_data_modtype74_count1.csv\n",
      "sensor_data_modtype1_count641.csv\n",
      "sensor_data_modtype26_count14.csv\n",
      "sensor_data_modtype25_count7.csv\n",
      "sensor_data_modtype34_count1.csv\n",
      "sensor_data_modtype67_count1.csv\n",
      "sensor_data_modtype11_count40.csv\n",
      "sensor_data_modtype64_count3.csv\n",
      "sensor_data_modtype9_count25.csv\n",
      "sensor_data_modtype6_count24.csv\n",
      "sensor_data_modtype66_count2.csv\n",
      "sensor_data_modtype57_count1.csv\n",
      "sensor_data_modtype0_count12.csv\n",
      "sensor_data_modtype18_count55.csv\n",
      "sensor_data_modtype51_count1.csv\n",
      "sensor_data_modtype10_count58.csv\n",
      "sensor_data_modtype81_count1.csv\n",
      "sensor_data_modtype86_count1.csv\n",
      "sensor_data_modtype5_count49.csv\n",
      "sensor_data_modtype20_count7.csv\n",
      "sensor_data_modtype60_count3.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#get the current working directory\n",
    "print(os.getcwd()) \n",
    "\n",
    "#list files in current working directory\n",
    "os.listdir(os.curdir)\n",
    "\n",
    "files = os.listdir(\"/azureml-run/data/modtype_sensor_data\")\n",
    "for file in files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sensor_data_modtype1_count641.csv']\n"
     ]
    }
   ],
   "source": [
    "# Path of the hourly sensor data\n",
    "data_path = \"/azureml-run/data/modtype_sensor_data\"\n",
    "# Look at a certain module type\n",
    "cur_modtype = SAMPLE_MODULE_TYPE\n",
    "matched_files = [file_name for file_name in os.listdir(data_path) \n",
    "                 if \"sensor_data_modtype{}_\".format(cur_modtype) in file_name]\n",
    "print(matched_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_data_modtype1_count641.csv\n"
     ]
    }
   ],
   "source": [
    "data_file_name = matched_files[0]\n",
    "print(data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/azureml-run/data/modtype_sensor_data/sensor_data_modtype1_count641.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path + '/' + data_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FF DRS</th>\n",
       "      <th>Input 1</th>\n",
       "      <th>Input 2</th>\n",
       "      <th>Input 3</th>\n",
       "      <th>label</th>\n",
       "      <th>modindex</th>\n",
       "      <th>modtype</th>\n",
       "      <th>rackindex</th>\n",
       "      <th>rests_hourly</th>\n",
       "      <th>storeno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.357143</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-15 15:00:00</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FF DRS  Input 1  Input 2  Input 3  label  modindex  modtype  rackindex  \\\n",
       "0  5.357143    -66.0    -66.0    -66.0      0        53        1          3   \n",
       "\n",
       "          rests_hourly  storeno  \n",
       "0  2017-05-15 15:00:00     1274  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_df = pd.read_csv(data_path + '/' + data_file_name)\n",
    "sensor_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FF DRS</th>\n",
       "      <th>Input 1</th>\n",
       "      <th>Input 2</th>\n",
       "      <th>Input 3</th>\n",
       "      <th>label</th>\n",
       "      <th>modindex</th>\n",
       "      <th>modtype</th>\n",
       "      <th>rackindex</th>\n",
       "      <th>rests_hourly</th>\n",
       "      <th>storeno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FF DRS, Input 1, Input 2, Input 3, label, modindex, modtype, rackindex, rests_hourly, storeno]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove outliers\n",
    "sensor_df[sensor_df['storeno'] < -900.0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90916, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FF DRS</th>\n",
       "      <th>Input 1</th>\n",
       "      <th>Input 2</th>\n",
       "      <th>Input 3</th>\n",
       "      <th>label</th>\n",
       "      <th>modindex</th>\n",
       "      <th>modtype</th>\n",
       "      <th>rackindex</th>\n",
       "      <th>rests_hourly</th>\n",
       "      <th>storeno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.357143</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-15 15:00:00</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FF DRS  Input 1  Input 2  Input 3  label  modindex  modtype  rackindex  \\\n",
       "0  5.357143    -66.0    -66.0    -66.0      0        53        1          3   \n",
       "\n",
       "          rests_hourly  storeno  \n",
       "0  2017-05-15 15:00:00     1274  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the following columns:\n",
    "# - labels indicating if an alarm will happen in the next X hour\n",
    "# - an id to uniquely identify each device\n",
    "# - month of the year, week of the month, day of the week\n",
    "def week_of_month(dt):\n",
    "    \"\"\" Returns the week of the month for the specified date.\n",
    "    \"\"\"\n",
    "    first_day = dt.replace(day=1)\n",
    "    dom = dt.day\n",
    "    adjusted_dom = dom + first_day.weekday()\n",
    "    return int(ceil(adjusted_dom/7.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FF DRS</th>\n",
       "      <th>Input 1</th>\n",
       "      <th>Input 2</th>\n",
       "      <th>Input 3</th>\n",
       "      <th>label</th>\n",
       "      <th>modindex</th>\n",
       "      <th>modtype</th>\n",
       "      <th>rackindex</th>\n",
       "      <th>rests_hourly</th>\n",
       "      <th>storeno</th>\n",
       "      <th>month_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>new_label</th>\n",
       "      <th>store_rack_module_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.357143</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-15 15:00:00</td>\n",
       "      <td>1274</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FF DRS  Input 1  Input 2  Input 3  label  modindex  modtype  rackindex  \\\n",
       "0  5.357143    -66.0    -66.0    -66.0      0        53        1          3   \n",
       "\n",
       "         rests_hourly  storeno  month_of_year  week_of_month day_of_week  \\\n",
       "0 2017-05-15 15:00:00     1274              5              3           1   \n",
       "\n",
       "   new_label  store_rack_module_id  \n",
       "0          0                     0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foresight_len = FORESIGHT_LEN_HOUR   # X = 8 \n",
    "storeno_list = list(set(sensor_df['storeno']))\n",
    "sensor_df['rests_hourly'] =  pd.to_datetime(sensor_df['rests_hourly'], format='%Y-%m-%dT%H:%M:%S')\n",
    "sensor_df['month_of_year'] = sensor_df['rests_hourly'].map(lambda x: x.month)\n",
    "sensor_df['week_of_month'] = sensor_df['rests_hourly'].map(lambda x: week_of_month(x))\n",
    "sensor_df['day_of_week'] = sensor_df['rests_hourly'].map(lambda x: x.strftime('%w'))\n",
    "sensor_df['new_label'] = sensor_df['label']\n",
    "sensor_df['store_rack_module_id'] = 0\n",
    "sensor_module_list = []\n",
    "count = 0\n",
    "sensor_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FF DRS</th>\n",
       "      <th>Input 1</th>\n",
       "      <th>Input 2</th>\n",
       "      <th>Input 3</th>\n",
       "      <th>label</th>\n",
       "      <th>modindex</th>\n",
       "      <th>modtype</th>\n",
       "      <th>rackindex</th>\n",
       "      <th>rests_hourly</th>\n",
       "      <th>storeno</th>\n",
       "      <th>month_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>new_label</th>\n",
       "      <th>store_rack_module_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.285714</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-15 15:00:00</td>\n",
       "      <td>1696</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FF DRS  Input 1  Input 2  Input 3  label  modindex  modtype  rackindex  \\\n",
       "0 -2.285714    -66.0    -66.0    -66.0      0        51        1          3   \n",
       "\n",
       "         rests_hourly  storeno  month_of_year  week_of_month day_of_week  \\\n",
       "0 2017-05-15 15:00:00     1696              5              3           1   \n",
       "\n",
       "   new_label  store_rack_module_id  \n",
       "0        0.0                     1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for cur_storeno in storeno_list:\n",
    "    sensor_store = sensor_df[sensor_df['storeno']==cur_storeno]\n",
    "    rackindex_list = list(set(sensor_store['rackindex']))\n",
    "    for cur_rackindex in rackindex_list:\n",
    "        sensor_rack = sensor_store[sensor_store['rackindex']==cur_rackindex]\n",
    "        modindex_list = list(set(sensor_rack['modindex']))\n",
    "        for cur_modindex in modindex_list:\n",
    "            count += 1\n",
    "            sensor_module = sensor_rack[sensor_rack['modindex']==cur_modindex]\n",
    "            sensor_module = sensor_module.sort_values(by='rests_hourly').reset_index(drop=True)\n",
    "            sensor_module['store_rack_module_id'] = count\n",
    "            alarm_index = list(sensor_module[sensor_module['label']==1].index)\n",
    "            if len(alarm_index) > 0:\n",
    "                new_label_index = alarm_index\n",
    "                for i in range(1,foresight_len):\n",
    "                    new_label_index = new_label_index + [max(idx-i,0) for idx in alarm_index]\n",
    "                    #print(new_label_index)\n",
    "                #sensor_module['new_label'][new_label_index] = 1.0\n",
    "                sensor_module.loc[new_label_index,'new_label'] = 1.0   # A better way than above\n",
    "                #print(list(sensor_module['new_label']))\n",
    "            sensor_module_list.append(sensor_module)\n",
    "sensor_df = pd.concat(sensor_module_list)\n",
    "sensor_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5020.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_df = sensor_df.reset_index(drop=True)\n",
    "sum(sensor_df['new_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FF DRS</th>\n",
       "      <th>Input 1</th>\n",
       "      <th>Input 2</th>\n",
       "      <th>Input 3</th>\n",
       "      <th>label</th>\n",
       "      <th>modindex</th>\n",
       "      <th>modtype</th>\n",
       "      <th>rackindex</th>\n",
       "      <th>rests_hourly</th>\n",
       "      <th>storeno</th>\n",
       "      <th>month_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>new_label</th>\n",
       "      <th>store_rack_module_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.285714</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-15 15:00:00</td>\n",
       "      <td>1696</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FF DRS  Input 1  Input 2  Input 3  label  modindex  modtype  rackindex  \\\n",
       "0 -2.285714    -66.0    -66.0    -66.0      0        51        1          3   \n",
       "\n",
       "         rests_hourly  storeno  month_of_year  week_of_month day_of_week  \\\n",
       "0 2017-05-15 15:00:00     1696              5              3           1   \n",
       "\n",
       "   new_label  store_rack_module_id  \n",
       "0        0.0                     1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into training and testing sets\n",
    "cur_dt = pd.to_datetime(PREDICT_HAPPEN_DT, format='%Y-%m-%dT%H:%M:%S')\n",
    "train_df = sensor_df[sensor_df['rests_hourly'] < cur_dt]\n",
    "test_df = sensor_df[sensor_df['rests_hourly'] >= cur_dt]\n",
    "buffer_df = sensor_df[(sensor_df['rests_hourly'] >= cur_dt - datetime.timedelta(hours=SEQUENCE_LEN_LSTM)) & \n",
    "                     (sensor_df['rests_hourly'] < cur_dt)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33088\n",
      "9024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(buffer_df))\n",
    "len(set(test_df['store_rack_module_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelling\n",
    "\n",
    "The traditional predictive maintenance machine learning models are based on feature engineering which is manual construction of right features using domain expertise and similar methods. This usually makes these models hard to reuse since feature engineering is specific to the problem scenario and the available data which varies from one business to the other. Perhaps the most attractive part of applying deep learning in the predictive maintenance domain is the fact that these networks can automatically extract the right features from the data, eliminating the need for manual feature engineering.\n",
    "\n",
    "When using LSTMs in the time-series domain, one important parameter to pick is the sequence length which is the window for LSTMs to look back. This may be viewed as similar to picking window_size = 5 cycles for calculating the rolling features in the [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) which are rolling mean and rolling standard deviation for 21 sensor values. The idea of using LSTMs is to let the model extract abstract features out of the sequence of sensor values in the window rather than engineering those manually. The expectation is that if there is a pattern in these sensor values within the window prior to failure, the pattern should be encoded by the LSTM.\n",
    "\n",
    "One critical advantage of LSTMs is their ability to remember from long-term sequences (window sizes) which is hard to achieve by traditional feature engineering. For example, computing rolling averages over a window size of 48 cycles may lead to loss of information due to smoothing and abstracting of values over such a long period, istead, using all 48 values as input may provide better results. While feature engineering over large window sizes may not make sense, LSTMs are able to use larger window sizes and use all the information in the window as input. Below, we illustrate the approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick a large window size of 48 hours\n",
    "sequence_length = SEQUENCE_LEN_LSTM\n",
    "sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras LSTM](https://keras.io/layers/recurrent/) layers expect an input in the shape of a numpy array of 3 dimensions (samples, time steps, features) where samples is the number of training sequences, time steps is the look back window or sequence length and features is the number of features of each sequence at each time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48804, 48, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sequences and convert to numpy array for training data\n",
    "sequence_cols = SEQUENCE_COLS_LSTM   \n",
    "seq_gen = (list(gen_sequence(train_df[train_df['store_rack_module_id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['store_rack_module_id'].unique())\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33088, 48, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sequences and convert to numpy array for testing data\n",
    "test_df2 = pd.concat([buffer_df, test_df]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "seq_gen_test = (list(gen_sequence(test_df2[test_df2['store_rack_module_id']==id], \n",
    "                                  sequence_length, sequence_cols)) \n",
    "                for id in test_df2['store_rack_module_id'].unique())\n",
    "seq_array_test = np.concatenate(list(seq_gen_test)).astype(np.float32)\n",
    "seq_array_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48804, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate labels for training data\n",
    "label_gen = [gen_labels(train_df[train_df['store_rack_module_id']==id], sequence_length, ['new_label']) \n",
    "             for id in train_df['store_rack_module_id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dt = train_df['rests_hourly'][0]+datetime.timedelta(hours=SEQUENCE_LEN_LSTM-1)\n",
    "\n",
    "train_df2 = train_df[train_df['rests_hourly'] > threshold_dt].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33088, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate labels for testing data\n",
    "label_gen_test = [gen_labels(test_df2[test_df2['store_rack_module_id']==id], sequence_length, ['new_label']) \n",
    "                  for id in test_df2['store_rack_module_id'].unique()]\n",
    "label_array_test = np.concatenate(label_gen_test).astype(np.float32)\n",
    "label_array_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Network\n",
    "Next, we build a deep network. The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. Dropout is also applied after each LSTM layer to control overfitting. Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_array.shape[2]\n",
    "label_array.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#model.add(LSTM(input_shape=(sequence_length, nb_features), units=100, return_sequences=True))\n",
    "model.add(LSTM(80, input_shape=(sequence_length, nb_features), return_sequences=True))\n",
    "model.add(Dropout(0.9))\n",
    "\n",
    "#model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(LSTM(40, return_sequences=False))\n",
    "model.add(Dropout(0.9))\n",
    "\n",
    "#model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "model.add(Dense(nb_out, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 48, 80)            27200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 80)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 40)                19360     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 46,601\n",
      "Trainable params: 46,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39043 samples, validate on 9761 samples\n",
      "Epoch 1/10\n",
      "39043/39043 [==============================] - 27s 702us/step - loss: 0.1911 - acc: 0.9588 - val_loss: 0.8622 - val_acc: 0.8059\n",
      "Epoch 2/10\n",
      "39043/39043 [==============================] - 27s 687us/step - loss: 0.1215 - acc: 0.9679 - val_loss: 0.5537 - val_acc: 0.8066\n",
      "Epoch 3/10\n",
      "39043/39043 [==============================] - 26s 675us/step - loss: 0.0995 - acc: 0.9694 - val_loss: 0.5000 - val_acc: 0.8126\n",
      "Epoch 4/10\n",
      "39043/39043 [==============================] - 27s 683us/step - loss: 0.0865 - acc: 0.9727 - val_loss: 0.6259 - val_acc: 0.8132\n",
      "CPU times: user 6min 57s, sys: 1min, total: 7min 58s\n",
      "Wall time: 1min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd25c5dfa58>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fit the network\n",
    "#model.fit(seq_array, label_array)\n",
    "model.fit(seq_array, label_array, nb_epoch=10, batch_size=200, validation_split=0.2, verbose=1, \n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48804/48804 [==============================] - 10s 199us/step\n",
      "Accuracy: 0.940763051979535\n"
     ]
    }
   ],
   "source": [
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('Accuracy: {}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48804/48804 [==============================] - 9s 194us/step\n"
     ]
    }
   ],
   "source": [
    "#model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "predicted_prob = model.predict_proba(seq_array,verbose=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90916, 15)\n",
      "(48804, 48, 4)\n",
      "(48804, 1)\n",
      "(33088, 48, 4)\n",
      "(33088, 1)\n"
     ]
    }
   ],
   "source": [
    "print(sensor_df.shape)\n",
    "print(seq_array.shape)\n",
    "print(label_array.shape)\n",
    "print(seq_array_test.shape)\n",
    "print(label_array_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9430307059961315\n"
     ]
    }
   ],
   "source": [
    "# test metrics\n",
    "scores_test = model.evaluate(seq_array_test, label_array_test, verbose=2)\n",
    "print('Accuracy: {}'.format(scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict_proba(seq_array_test)\n",
    "y_pred_prob = y_pred_prob/max(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "- x-axis is true labels.\n",
      "- y-axis is predicted labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[27921,  3534],\n",
       "       [  199,  1434]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "#y_pred_test = model.predict_classes(seq_array_test)\n",
    "y_pred_test = y_pred_prob > 0.03\n",
    "y_true_test = label_array_test\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.288647342995 \n",
      " Recall:  0.878138395591 \n",
      " F1-score: 0.434479624299\n"
     ]
    }
   ],
   "source": [
    "# compute precision and recall\n",
    "precision_test = precision_score(y_true_test, y_pred_test)\n",
    "recall_test = recall_score(y_true_test, y_pred_test)\n",
    "f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
    "print( 'Precision: ', precision_test, '\\n', 'Recall: ', recall_test,'\\n', 'F1-score:', f1_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/loading the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a copy of the file\n",
    "model_cp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7fd25c392780>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and test the loaded model locally first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for operationalization: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "import os\n",
    "import h5py\n",
    "from sklearn import datasets \n",
    " \n",
    "# save model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"modellstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"modellstm.h5\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('modellstm.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"modellstm.h5\")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48804/48804 [==============================] - 21s 430us/step\n",
      "(48804, 1)\n",
      "[[  7.79028996e-05]\n",
      " [  8.52339363e-05]\n",
      " [  9.56085496e-05]\n",
      " ..., \n",
      " [  1.41510085e-04]\n",
      " [  1.82924763e-04]\n",
      " [  2.65550625e-04]]\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.predict_proba(seq_array,verbose=1)\n",
    "print(score.shape)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the files to the shared folder for operationalization\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files\n",
    "\n",
    "#### Ubuntu Linux\n",
    "/home/<username>/ then navigate to .azureml/share/<exp_acct_name>/<workspace_name>/<proj_name>/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json file written shared folder\n"
     ]
    }
   ],
   "source": [
    "with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'modellstm.json', 'wt') as json_file:\n",
    "    json_file.write(model_json)\n",
    "    print(\"json file written shared folder\")\n",
    "    json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cp.save_weights(os.path.join(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'], 'modellstm.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the init() and run() functions and testing them locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# import the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Setting seed for reproducability\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the init() and run() functions to read from the shared folder location: https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test reading in the files from the shared folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json file read from shared folder\n"
     ]
    }
   ],
   "source": [
    "with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'modellstm.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    print(\"json file read from shared folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights read from shared folder\n"
     ]
    }
   ],
   "source": [
    "with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'modellstm.h5', 'r') as model:\n",
    "    loaded_model.load_weights(\"modellstm.h5\")\n",
    "    print(\"model weights read from shared folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.79028996e-05],\n",
       "       [  8.52339363e-05],\n",
       "       [  9.56085496e-05],\n",
       "       ..., \n",
       "       [  1.41510085e-04],\n",
       "       [  1.82924763e-04],\n",
       "       [  2.65550625e-04]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = loaded_model.predict_proba(seq_array)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test init() and run() functions to read from the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('modellstm.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"modellstm.h5\")\n",
    "    print(\"Loaded model\")\n",
    "    \n",
    "    #inputs_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"inputs\")\n",
    "    #print(inputs_dc)\n",
    "    #prediction_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48804, 48, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]\n",
    "\n",
    "# generate sequences and convert to numpy array for training data\n",
    "sequence_cols = SEQUENCE_COLS_LSTM   \n",
    "seq_gen = (list(gen_sequence(train_df[train_df['store_rack_module_id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['store_rack_module_id'].unique())\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape\n",
    "#seq_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(seq_array): \n",
    "    global clf2, inputs_dc, prediction_dc\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        #print(prediction)\n",
    "        #inputs_dc.collect(seq_array)\n",
    "        #prediction_dc.collect(prediction)\n",
    "        return (prediction)\n",
    "    except Exception as e:\n",
    "        return(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.79028996e-05],\n",
       "       [  8.52339363e-05],\n",
       "       [  9.56085496e-05],\n",
       "       ..., \n",
       "       [  1.41510085e-04],\n",
       "       [  1.82924763e-04],\n",
       "       [  2.65550625e-04]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(seq_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the init() and run() functions to read from the shared directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    # load json and create model\n",
    "    with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'modellstm.json', 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        print(\"json file read from shared folder\")\n",
    "    # load weights into new model\n",
    "    with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'modellstm.h5', 'r') as model:\n",
    "        loaded_model.load_weights(\"modellstm.h5\")\n",
    "        print(\"model weights read from shared folder\")\n",
    "        print(\"Loaded model\")\n",
    "    \n",
    "    #inputs_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"inputs\")\n",
    "    #print(inputs_dc)\n",
    "    #prediction_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"prediction\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(seq_array): \n",
    "    global clf2, inputs_dc, prediction_dc\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        #print(prediction)\n",
    "        #inputs_dc.collect(seq_array)\n",
    "        #prediction_dc.collect(prediction)\n",
    "        return (prediction)\n",
    "    except Exception as e:\n",
    "        return(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json file read from shared folder\n",
      "model weights read from shared folder\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.79028996e-05],\n",
       "       [  8.52339363e-05],\n",
       "       [  9.56085496e-05],\n",
       "       ..., \n",
       "       [  1.41510085e-04],\n",
       "       [  1.82924763e-04],\n",
       "       [  2.65550625e-04]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(seq_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model assets\n",
    "\n",
    "Next we persist the assets we have created to disk for use in operationalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /azureml-share//lstmscore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']}/lstmscore.py\n",
    "\n",
    "import keras\n",
    "# import the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Setting seed for reproducability\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('modellstm.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"modellstm.h5\")\n",
    "    print(\"Loaded model\")\n",
    "    \n",
    "    #inputs_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"inputs\")\n",
    "    #print(inputs_dc)\n",
    "    #prediction_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"prediction\")\n",
    "    \n",
    "    \n",
    "def run(seq_array): \n",
    "    global clf2, inputs_dc, prediction_dc\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        #print(prediction)\n",
    "        #inputs_dc.collect(seq_array)\n",
    "        #prediction_dc.collect(prediction)\n",
    "        return (prediction)\n",
    "    except Exception as e:\n",
    "        return(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the files from the shared folder:\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DeepLearningforPM DeepLearningforPM",
   "language": "python",
   "name": "deeplearningforpm_deeplearningforpm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
